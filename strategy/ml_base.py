"""
Base classes for Machine Learning Trading Strategies
Provides common infrastructure for ML/DL based trading strategies
"""
from __future__ import annotations
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass
from abc import ABC, abstractmethod
from enum import Enum
import pickle
import json
from pathlib import Path


class PredictionType(Enum):
    """Type of prediction the model makes"""
    CLASSIFICATION = "classification"  # Buy/Sell/Hold
    REGRESSION = "regression"          # Price prediction
    RANKING = "ranking"                # Relative ranking


class ModelState(Enum):
    """Model training state"""
    UNTRAINED = "untrained"
    TRAINING = "training"
    TRAINED = "trained"
    DEPLOYED = "deployed"


@dataclass
class MLSignal:
    """Signal generated by ML model"""
    symbol: str
    prediction: Union[int, float, np.ndarray]
    confidence: float
    signal_strength: float  # -1 to 1
    prediction_type: PredictionType
    feature_importance: Optional[Dict[str, float]] = None
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class TrainingConfig:
    """Configuration for model training"""
    train_start_date: str
    train_end_date: str
    validation_split: float = 0.2
    test_split: float = 0.1
    batch_size: int = 32
    epochs: int = 100
    early_stopping_patience: int = 10
    learning_rate: float = 0.001
    random_seed: int = 42
    use_gpu: bool = True


class FeatureEngineer:
    """
    Feature engineering for trading ML models
    Generates technical, fundamental, and market microstructure features
    """

    def __init__(self, lookback_periods: List[int] = [5, 10, 20, 60]):
        self.lookback_periods = lookback_periods
        self.feature_names = []

    def generate_features(self, df: pd.DataFrame, symbol: str = None) -> pd.DataFrame:
        """Generate all features from raw price data"""
        features_df = df.copy()

        # Price-based features
        features_df = self._add_price_features(features_df)

        # Technical indicators
        features_df = self._add_technical_indicators(features_df)

        # Volume features
        features_df = self._add_volume_features(features_df)

        # Microstructure features
        features_df = self._add_microstructure_features(features_df)

        # Time-based features
        features_df = self._add_time_features(features_df)

        # Lag features
        features_df = self._add_lag_features(features_df)

        # Store feature names
        self.feature_names = [col for col in features_df.columns if col not in df.columns]

        return features_df

    def _add_price_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add price-based features"""
        for period in self.lookback_periods:
            # Returns
            df[f'return_{period}'] = df['close'].pct_change(period)

            # Log returns
            df[f'log_return_{period}'] = np.log(df['close'] / df['close'].shift(period))

            # Price momentum
            df[f'momentum_{period}'] = df['close'] - df['close'].shift(period)

            # Price position in range
            df[f'price_position_{period}'] = (
                (df['close'] - df['low'].rolling(period).min()) /
                (df['high'].rolling(period).max() - df['low'].rolling(period).min())
            )

        # High-low spread
        df['hl_spread'] = (df['high'] - df['low']) / df['close']

        # Close position in daily range
        df['close_position'] = (df['close'] - df['low']) / (df['high'] - df['low'])

        return df

    def _add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add technical indicators"""
        # Moving averages
        for period in self.lookback_periods:
            df[f'sma_{period}'] = df['close'].rolling(period).mean()
            df[f'ema_{period}'] = df['close'].ewm(span=period).mean()

        # MACD
        ema12 = df['close'].ewm(span=12).mean()
        ema26 = df['close'].ewm(span=26).mean()
        df['macd'] = ema12 - ema26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']

        # RSI
        delta = df['close'].diff()
        gain = delta.clip(lower=0)
        loss = (-delta).clip(lower=0)

        for period in [14, 28]:
            avg_gain = gain.rolling(period).mean()
            avg_loss = loss.rolling(period).mean()
            rs = avg_gain / avg_loss
            df[f'rsi_{period}'] = 100 - (100 / (1 + rs))

        # Bollinger Bands
        for period in [20, 50]:
            sma = df['close'].rolling(period).mean()
            std = df['close'].rolling(period).std()
            df[f'bb_upper_{period}'] = sma + 2 * std
            df[f'bb_lower_{period}'] = sma - 2 * std
            df[f'bb_width_{period}'] = (df[f'bb_upper_{period}'] - df[f'bb_lower_{period}']) / sma
            df[f'bb_position_{period}'] = (df['close'] - df[f'bb_lower_{period}']) / (df[f'bb_upper_{period}'] - df[f'bb_lower_{period}'])

        # ATR (Average True Range)
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        df['atr_14'] = true_range.rolling(14).mean()

        # ADX (Average Directional Index)
        df['adx_14'] = self._calculate_adx(df, period=14)

        return df

    def _add_volume_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add volume-based features"""
        for period in self.lookback_periods:
            # Volume moving average
            df[f'volume_sma_{period}'] = df['volume'].rolling(period).mean()

            # Volume ratio
            df[f'volume_ratio_{period}'] = df['volume'] / df[f'volume_sma_{period}']

        # On-Balance Volume (OBV)
        df['obv'] = (np.sign(df['close'].diff()) * df['volume']).fillna(0).cumsum()

        # Volume-Weighted Average Price (VWAP)
        df['vwap'] = (df['close'] * df['volume']).cumsum() / df['volume'].cumsum()

        # Money Flow Index
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        money_flow = typical_price * df['volume']

        positive_flow = money_flow.where(typical_price > typical_price.shift(1), 0)
        negative_flow = money_flow.where(typical_price < typical_price.shift(1), 0)

        mfi_period = 14
        positive_mf = positive_flow.rolling(mfi_period).sum()
        negative_mf = negative_flow.rolling(mfi_period).sum()
        mfi_ratio = positive_mf / negative_mf
        df['mfi'] = 100 - (100 / (1 + mfi_ratio))

        return df

    def _add_microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add market microstructure features"""
        # Bid-ask spread proxy (high-low)
        df['spread_proxy'] = df['high'] - df['low']

        # Price impact
        df['price_impact'] = df['close'].diff() / df['volume']

        # Volatility measures
        for period in self.lookback_periods:
            df[f'volatility_{period}'] = df['close'].pct_change().rolling(period).std()
            df[f'parkinson_vol_{period}'] = np.sqrt(
                (1 / (4 * period * np.log(2))) *
                ((np.log(df['high'] / df['low']) ** 2).rolling(period).sum())
            )

        return df

    def _add_time_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add time-based features"""
        if isinstance(df.index, pd.DatetimeIndex):
            df['day_of_week'] = df.index.dayofweek
            df['day_of_month'] = df.index.day
            df['month'] = df.index.month
            df['quarter'] = df.index.quarter

            # Cyclical encoding
            df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
            df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
            df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
            df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)

        return df

    def _add_lag_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add lagged features"""
        lag_periods = [1, 2, 3, 5, 10]
        lag_columns = ['close', 'volume', 'return_5', 'rsi_14']

        for col in lag_columns:
            if col in df.columns:
                for lag in lag_periods:
                    df[f'{col}_lag_{lag}'] = df[col].shift(lag)

        return df

    def _calculate_adx(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """Calculate Average Directional Index"""
        high_diff = df['high'].diff()
        low_diff = -df['low'].diff()

        pos_dm = high_diff.where((high_diff > low_diff) & (high_diff > 0), 0)
        neg_dm = low_diff.where((low_diff > high_diff) & (low_diff > 0), 0)

        atr = self._calculate_atr(df, period)

        pos_di = 100 * pos_dm.rolling(period).mean() / atr
        neg_di = 100 * neg_dm.rolling(period).mean() / atr

        dx = 100 * np.abs(pos_di - neg_di) / (pos_di + neg_di)
        adx = dx.rolling(period).mean()

        return adx

    def _calculate_atr(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """Calculate Average True Range"""
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        return true_range.rolling(period).mean()

    def get_feature_names(self) -> List[str]:
        """Get list of generated feature names"""
        return self.feature_names


class BaseMLStrategy(ABC):
    """
    Abstract base class for ML-based trading strategies
    """

    def __init__(self,
                 name: str,
                 prediction_type: PredictionType,
                 lookback_window: int = 60,
                 feature_engineer: Optional[FeatureEngineer] = None,
                 model_path: Optional[str] = None):
        self.name = name
        self.prediction_type = prediction_type
        self.lookback_window = lookback_window
        self.feature_engineer = feature_engineer or FeatureEngineer()
        self.model_path = model_path
        self.state = ModelState.UNTRAINED
        self.model = None
        self.scaler = None
        self.training_history = []
        self.feature_importance = {}

    @abstractmethod
    def build_model(self, input_shape: Tuple[int, ...], **kwargs):
        """Build the ML/DL model architecture"""
        pass

    @abstractmethod
    def train(self,
              X_train: np.ndarray,
              y_train: np.ndarray,
              X_val: Optional[np.ndarray] = None,
              y_val: Optional[np.ndarray] = None,
              **kwargs) -> Dict[str, Any]:
        """Train the model"""
        pass

    @abstractmethod
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Make predictions"""
        pass

    def prepare_data(self,
                     price_data: Dict[str, pd.DataFrame],
                     target_column: str = 'target',
                     train_split: float = 0.7,
                     val_split: float = 0.15) -> Tuple[np.ndarray, ...]:
        """
        Prepare data for training
        Returns: X_train, X_val, X_test, y_train, y_val, y_test
        """
        all_features = []
        all_targets = []

        for symbol, df in price_data.items():
            # Generate features
            features_df = self.feature_engineer.generate_features(df, symbol)

            # Create target variable
            if target_column not in features_df.columns:
                features_df = self._create_target(features_df)

            # Drop NaN values
            features_df = features_df.dropna()

            # Separate features and target
            feature_cols = self.feature_engineer.get_feature_names()
            X = features_df[feature_cols].values
            y = features_df[target_column].values

            all_features.append(X)
            all_targets.append(y)

        # Concatenate all data
        X = np.vstack(all_features)
        y = np.concatenate(all_targets)

        # Split data
        n = len(X)
        train_end = int(n * train_split)
        val_end = int(n * (train_split + val_split))

        X_train, y_train = X[:train_end], y[:train_end]
        X_val, y_val = X[train_end:val_end], y[train_end:val_end]
        X_test, y_test = X[val_end:], y[val_end:]

        # Scale features
        from sklearn.preprocessing import StandardScaler
        self.scaler = StandardScaler()
        X_train = self.scaler.fit_transform(X_train)
        X_val = self.scaler.transform(X_val)
        X_test = self.scaler.transform(X_test)

        return X_train, X_val, X_test, y_train, y_val, y_test

    def _create_target(self, df: pd.DataFrame, horizon: int = 1) -> pd.DataFrame:
        """Create target variable for prediction"""
        if self.prediction_type == PredictionType.CLASSIFICATION:
            # Classification: 0=sell, 1=hold, 2=buy
            future_return = df['close'].pct_change(horizon).shift(-horizon)
            target_cut = pd.cut(
                future_return,
                bins=[-np.inf, -0.01, 0.01, np.inf],
                labels=[0, 1, 2]
            )
            # Convert to int, handling NaN by filling with 1 (hold)
            df['target'] = pd.to_numeric(target_cut, errors='coerce').fillna(1).astype(int)

        elif self.prediction_type == PredictionType.REGRESSION:
            # Regression: predict future return
            df['target'] = df['close'].pct_change(horizon).shift(-horizon)

        elif self.prediction_type == PredictionType.RANKING:
            # Ranking: relative return rank
            future_return = df['close'].pct_change(horizon).shift(-horizon)
            df['target'] = future_return.rank(pct=True)

        return df

    def generate_signals(self, price_data: Dict[str, pd.DataFrame]) -> Dict[str, MLSignal]:
        """Generate trading signals from predictions"""
        if self.state != ModelState.TRAINED and self.state != ModelState.DEPLOYED:
            raise ValueError("Model must be trained before generating signals")

        signals = {}

        for symbol, df in price_data.items():
            # Generate features
            features_df = self.feature_engineer.generate_features(df, symbol)
            features_df = features_df.dropna()

            if len(features_df) == 0:
                continue

            # Get features
            feature_cols = self.feature_engineer.get_feature_names()
            X = features_df[feature_cols].values

            # Scale features
            if self.scaler is not None:
                X = self.scaler.transform(X)

            # Make prediction
            prediction = self.predict(X[-1:])

            # Convert prediction to signal
            signal = self._prediction_to_signal(symbol, prediction)
            signals[symbol] = signal

        return signals

    def _prediction_to_signal(self, symbol: str, prediction: np.ndarray) -> MLSignal:
        """Convert model prediction to trading signal"""
        if self.prediction_type == PredictionType.CLASSIFICATION:
            # prediction is class probabilities
            signal_strength = float(prediction[0][2] - prediction[0][0])  # buy prob - sell prob
            confidence = float(np.max(prediction[0]))
            pred_class = int(np.argmax(prediction[0]))

        elif self.prediction_type == PredictionType.REGRESSION:
            # prediction is future return
            signal_strength = float(np.tanh(prediction[0] * 10))  # normalize to [-1, 1]
            confidence = min(abs(signal_strength), 1.0)
            pred_class = prediction[0]

        elif self.prediction_type == PredictionType.RANKING:
            # prediction is rank score
            signal_strength = float(prediction[0] * 2 - 1)  # convert [0,1] to [-1,1]
            confidence = abs(signal_strength)
            pred_class = prediction[0]

        return MLSignal(
            symbol=symbol,
            prediction=pred_class,
            confidence=confidence,
            signal_strength=signal_strength,
            prediction_type=self.prediction_type,
            feature_importance=self.feature_importance
        )

    def save_model(self, path: Optional[str] = None):
        """Save model to disk"""
        save_path = path or self.model_path
        if save_path is None:
            raise ValueError("No save path specified")

        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)

        # Save model-specific data
        self._save_model_specific(save_path)

        # Save metadata
        metadata = {
            'name': self.name,
            'prediction_type': self.prediction_type.value,
            'lookback_window': self.lookback_window,
            'state': self.state.value,
            'feature_importance': self.feature_importance,
            'training_history': self.training_history
        }

        with open(save_path.with_suffix('.meta.json'), 'w') as f:
            json.dump(metadata, f, indent=2)

        # Save scaler
        if self.scaler is not None:
            with open(save_path.with_suffix('.scaler.pkl'), 'wb') as f:
                pickle.dump(self.scaler, f)

    def load_model(self, path: Optional[str] = None):
        """Load model from disk"""
        load_path = path or self.model_path
        if load_path is None:
            raise ValueError("No load path specified")

        load_path = Path(load_path)

        # Load model-specific data
        self._load_model_specific(load_path)

        # Load metadata
        with open(load_path.with_suffix('.meta.json'), 'r') as f:
            metadata = json.load(f)

        self.name = metadata['name']
        self.prediction_type = PredictionType(metadata['prediction_type'])
        self.lookback_window = metadata['lookback_window']
        self.state = ModelState(metadata['state'])
        self.feature_importance = metadata['feature_importance']
        self.training_history = metadata['training_history']

        # Load scaler
        scaler_path = load_path.with_suffix('.scaler.pkl')
        if scaler_path.exists():
            with open(scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)

        self.state = ModelState.DEPLOYED

    @abstractmethod
    def _save_model_specific(self, path: Path):
        """Save model-specific data (to be implemented by subclasses)"""
        pass

    @abstractmethod
    def _load_model_specific(self, path: Path):
        """Load model-specific data (to be implemented by subclasses)"""
        pass

    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:
        """Evaluate model performance"""
        predictions = self.predict(X_test)

        metrics = {}

        if self.prediction_type == PredictionType.CLASSIFICATION:
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            pred_classes = np.argmax(predictions, axis=1)

            metrics['accuracy'] = accuracy_score(y_test, pred_classes)
            metrics['precision'] = precision_score(y_test, pred_classes, average='weighted')
            metrics['recall'] = recall_score(y_test, pred_classes, average='weighted')
            metrics['f1'] = f1_score(y_test, pred_classes, average='weighted')

        elif self.prediction_type == PredictionType.REGRESSION:
            from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

            metrics['mse'] = mean_squared_error(y_test, predictions)
            metrics['mae'] = mean_absolute_error(y_test, predictions)
            metrics['r2'] = r2_score(y_test, predictions)
            metrics['rmse'] = np.sqrt(metrics['mse'])

        return metrics
